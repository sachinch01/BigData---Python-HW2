{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "spark = SparkSession.builder.appName('Homework').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data = pd.read_csv('C:/Users/Sachin Chavan/Downloads/Amazon_Responded_Oct05.csv') #importing the dataset\n",
    "amazon_data.head() #viewing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amazon_data.shape) #checking the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94283447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amazon_data.columns) #checking column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amazon_data.info()) #gathering information regarding our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a94312",
   "metadata": {},
   "source": [
    "#Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Remove the records where “user_verified” is “FALSE”. \n",
    "verified = amazon_data[amazon_data['user_verified'] == True] #filtering out the 'True' users in the dataset\n",
    "verified['user_verified'] #displaying the filtered column to check if the filter has been set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c886cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: For the remaining records (“user_verified” is “TRUE”), group by created date, and count the number of tweets for each date. \n",
    "\n",
    "verified['Month'] = pd.DatetimeIndex(verified['tweet_created_at']).month #using datetime function of pandas to extarct month\n",
    "\n",
    "#import calender\n",
    "#verified_true['Month'] = verified_true.apply(lambda x : calender.month_abbr[x])\n",
    "\n",
    "verified['Month'] = pd.to_datetime(verified['Month'], format='%m').dt.month_name().str.slice(stop=3) #converting it to string format to match the question \n",
    "verified['Day'] = pd.DatetimeIndex(verified['tweet_created_at']).day #extracting day using pandas datetime function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ba234",
   "metadata": {},
   "outputs": [],
   "source": [
    "verified['tweet_created_date'] = verified['Month'] + \" \" + verified['Day'].astype(str) #concatinating the two columns\n",
    "verified.head() #checking the final result of our date extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444357f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweet = verified.groupby(['tweet_created_date']).size().reset_index(name='Number of Tweets') #Had to name the column to help call it during sorting process\n",
    "count_tweet #checking to see if the number of tweets have been tabulated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: For the date with highest number of tweets (you can figure it out from step 2), calculate the sum of “favorite_count” and “retweet_count” for each tweet on that day. \n",
    "count_tweet_count = count_tweet.sort_values(by='Number of Tweets', ascending=False) #sorting the columns by 'Number of Tweets'\n",
    "count_tweet_count #We get to know the Jan 3 had the highest number of tweets, i.e. 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_tweet = verified[verified.tweet_created_date == 'Jan 3'] #extracting tweets pertinent to Jan 3 from verified_true\n",
    "highest_tweet #checking to see if all 1536 tweets have been extracted\n",
    "\n",
    "highest_tweet['tweet_sum'] = highest_tweet['favorite_count'] + highest_tweet['retweet_count'] #calculating the sum of the two columns and tabulating it in a new column 'tweet_sum'\n",
    "highest_tweet #checking to see if the data has been tabulated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Then report the text content (“text_”) of the top 100 tweets with highest sum. \n",
    "highest_tweet = highest_tweet.sort_values(by = 'tweet_sum', ascending = False)\n",
    "highest_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b095fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Count the word frequency of the 100 tweets and report the result (Note that data cleaning steps before wordcount can be done outside of Spark operations).\n",
    "highest_tweet_100 = highest_tweet.nlargest(100, 'tweet_sum') #using the 'nlargest' function in pandas to find the top 100 tweets sorted by 'tweet_sum'\n",
    "highest_tweet_100 = highest_tweet_100[['tweet_sum', 'text_']] #we add the 'text_' function to add the text to the data frame as well\n",
    "highest_tweet_100 #checking if the operation is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark= SparkSession.builder.getOrCreate()\n",
    "#spDF = SQLContext.createDataFrame(highest_tweet_100)\n",
    "#rdd = sc.parallelize([highest_tweet_100])\n",
    "\n",
    "text = \"\" #creating a variable to store the list, this would make it easier to pass it through RDD\n",
    "for i in range (0,100):\n",
    "    text = text + highest_tweet_100['text_'].values[i] #passing the list to the empty variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3: Count the word frequency of the 100 tweets and report the result\n",
    "sc =SparkContext.getOrCreate()\n",
    "text_file = sc.parallelize([text]) #passing the text list\n",
    "word_count = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).collect() #using the word-count function of pyspark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_count) #printing the list generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7116587",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_count) #ensuring type of 'word_count' to convert to spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06baf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Word\", \"Frequency\"] #defining the column names in our output\n",
    "word_count_table = spark.createDataFrame(data = word_count, schema = columns) #converting the list to Spark dataframe\n",
    "word_count_table.show() #displaying the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6efe01",
   "metadata": {},
   "source": [
    "#Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991bf710",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#You will use find_text.csv for this task. There are two columns in this document: “id_str” and “text”. The second column is empty. Please find out the text content of each tweet according to “id_str” joining Amazon_Responded_Oct05.csv and fill in the “text” column.\n",
    "find_text = pd.read_csv('C:/Users/Sachin Chavan/Downloads/find_text.csv') #importing find_text file in pandas dataframe\n",
    "find_text.head() #checking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ddb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spark = spark.createDataFrame(find_text) #converting find_text to spark dataframe\n",
    "amazon_string = amazon_data.astype(str) #we need to convert the dataframe to string to avoid errors when converting to spark dataframe\n",
    "amazon_spark = spark.createDataFrame(amazon_string) #converting amazon_data to spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_spark) #re-verifying if the conversion has been successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b57c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(amazon_spark) #re-verifying if the conversion has been successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = amazon_spark.join(text_spark,'id_str').select('id_str','text_') #using the join function of pyspark\n",
    "final_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ce559",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.toPandas().to_csv(('C:/Users/Sachin Chavan/Downloads/HW2_sachin.csv'), index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d1b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
